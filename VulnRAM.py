import requests
import re
import argparse
import os
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

def about():
    """Ø¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­ÙˆÙ„ Ø§Ù„Ø£Ø¯Ø§Ø©"""
    print("""
    ====================================
        Vuln_RAM - WordPress  AND WEP  Scanner
    ====================================
    Version: V1
    Developed by: Ramzey Elsayed Mohamed
    ------------------------------------
    An advanced Python tool for scanning and testing WordPress vulnerabilities,
    including XSS, SQL Injection, and user enumeration. 
    It also crawls the website and extracts potential parameters.
    ------------------------------------
    Usage:
    python vuln_ram.py https://target.com -o results.txt
    ------------------------------------
    ğŸš€ Stay Secure - Stay Updated!
    """)

def detect_wp_version(url):
    """ÙŠØ­Ø§ÙˆÙ„ Ø§ÙƒØªØ´Ø§Ù Ø¥ØµØ¯Ø§Ø± ÙˆÙˆØ±Ø¯Ø¨Ø±ÙŠØ³ Ø¨Ø¹Ø¯Ø© Ø·Ø±Ù‚"""
    version_sources = [
        (url, 'Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©'),
        (f"{url}/readme.html", 'readme.html'),
        (f"{url}/wp-includes/version.php", 'version.php')
    ]
    
    for link, source in version_sources:
        response = requests.get(link, headers={'User-Agent': 'Mozilla/5.0'})
        version = re.search(r'WordPress (\d+\.\d+\.\d+)', response.text)
        if version:
            return f"[INFO] Ø¥ØµØ¯Ø§Ø± ÙˆÙˆØ±Ø¯Ø¨Ø±ÙŠØ³ ({source}): {version.group(1)}"
    return "[WARNING] ØªØ¹Ø°Ø± Ø§ÙƒØªØ´Ø§Ù Ø¥ØµØ¯Ø§Ø± ÙˆÙˆØ±Ø¯Ø¨Ø±ÙŠØ³."

def extract_parameters(url):
    """ÙŠØ³ØªØ®Ø±Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨Ø±Ø§Ù…ØªØ±Ø§Øª Ø§Ù„Ù…Ù…ÙƒÙ†Ø© Ù…Ù† ØµÙØ­Ø§Øª Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
    response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
    params = re.findall(r'[?&]([a-zA-Z0-9_-]+)=', response.text)
    return list(set(params))

def check_xss(url, params):
    """ÙŠØ®ØªØ¨Ø± XSS Ø¹Ø¨Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨Ø±Ø§Ù…ØªØ±Ø§Øª Ø§Ù„Ù…Ù…ÙƒÙ†Ø©"""
    xss_payloads = ["<script>alert('XSS')</script>", "<img src=x onerror=alert('XSS')>", "\"><svg/onload=alert(1)>"]
    
    for param in params:
        for payload in xss_payloads:
            test_url = f"{url}/?{param}={payload}"
            response = requests.get(test_url, headers={'User-Agent': 'Mozilla/5.0'})
            if payload in response.text:
                return f"[XSS] Ù…Ø­ØªÙ…Ù„ ÙÙŠ {test_url}"
    return "[INFO] Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ XSS Ù…Ø¨Ø§Ø´Ø±."

def check_sqli(url, params):
    """ÙŠØ®ØªØ¨Ø± SQL Injection Ø¹Ø¨Ø± Ø§Ù„Ø¨Ø±Ø§Ù…ØªØ±Ø§Øª Ø§Ù„Ù…Ù…ÙƒÙ†Ø©"""
    sqli_payloads = ["' OR '1'='1", "' UNION SELECT null,null--", "' AND 1=1 --", "1' OR '1'='1"]
    
    for param in params:
        for payload in sqli_payloads:
            test_url = f"{url}/?{param}={payload}"
            response = requests.get(test_url, headers={'User-Agent': 'Mozilla/5.0'})
            if "mysql" in response.text.lower() or "sql" in response.text.lower():
                return f"[SQLi] Ù…Ø­ØªÙ…Ù„ ÙÙŠ {test_url}"
    return "[INFO] Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ SQL Injection Ù…Ø¨Ø§Ø´Ø±."

def check_users_enum(url):
    """ÙŠØ­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¹Ø¨Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ù…Ù…ÙƒÙ†Ø©"""
    users = []
    for i in range(1, 20):
        test_url = f"{url}/?author={i}"
        response = requests.get(test_url, allow_redirects=False, headers={'User-Agent': 'Mozilla/5.0'})
        if response.status_code in [301, 302]:
            user = re.search(r'/author/(.*)/', response.headers.get('Location', ''))
            if user and user.group(1) not in users:
                users.append(user.group(1))
    
    wp_json_url = f"{url}/wp-json/wp/v2/users"
    response = requests.get(wp_json_url, headers={'User-Agent': 'Mozilla/5.0'})
    if response.status_code == 200:
        json_users = re.findall(r'"name":"(.*?)"', response.text)
        users.extend(json_users)
    
    return "[INFO] Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†: " + ", ".join(set(users)) if users else "[INFO] Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ø³Ù…Ø§Ø¡ Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†."

def get_links(url, start_url):
    """ÙŠØ¬Ù„Ø¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© Ù„Ù„Ù…ÙˆÙ‚Ø¹"""
    try:
        response = requests.get(url, timeout=10)
        if response.status_code != 200:
            return []
        soup = BeautifulSoup(response.text, "html.parser")
        links = set()
        for a_tag in soup.find_all("a", href=True):
            full_url = urljoin(url, a_tag["href"])
            if urlparse(full_url).netloc == urlparse(start_url).netloc:
                links.add(full_url)
        return links
    except Exception as e:
        print(f"[Ø®Ø·Ø£] Ù„Ù… ÙŠØªÙ…ÙƒÙ† Ù…Ù† Ø¬Ù„Ø¨ {url}: {e}")
        return []

def crawl(url, start_url, visited_urls, results_file):
    """ÙŠÙ‚ÙˆÙ… Ø¨Ø§Ù„Ø²Ø­Ù Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆÙ‚Ø¹ ÙˆØ§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø«ØºØ±Ø§Øª"""
    if url in visited_urls:
        return
    print(f"[Ø²ÙŠØ§Ø±Ø©] {url}")
    visited_urls.add(url)
    links = get_links(url, start_url)
    
    with open(results_file, "a", encoding="utf-8") as file:
        results = []
        results.append(detect_wp_version(url))
        all_params = extract_parameters(url)
        results.append(check_xss(url, all_params))
        results.append(check_sqli(url, all_params))
        results.append(check_users_enum(url))
        
        for result in results:
            if result:
                file.write(result + "\n")
        
    for link in links:
        time.sleep(1)  # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ· Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±
        crawl(link, start_url, visited_urls, results_file)

def main():
    parser = argparse.ArgumentParser(description="Ø£Ø¯Ø§Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„ÙØ­Øµ Ø«ØºØ±Ø§Øª ÙˆÙˆØ±Ø¯Ø¨Ø±ÙŠØ³ ÙˆØ§Ù„Ø²Ø­Ù Ø¹Ø¨Ø± Ø§Ù„Ø±ÙˆØ§Ø¨Ø·")
    parser.add_argument("url", help="Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù„Ø¨Ø¯Ø¡ Ø§Ù„Ø²Ø­Ù ÙˆØ§Ù„ÙØ­Øµ")
    parser.add_argument("-o", "--output", default="results.txt", help="Ù…Ù„Ù Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ù„Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬")
    args = parser.parse_args()
    
    visited_urls = set()
    crawl(args.url, args.url, visited_urls, args.output)
    print(f"[Ø§Ù†ØªÙ‡Ø§Ø¡] ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ {args.output}")

if __name__ == "__main__":
    main()
